{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Proceso ETL - Carga de Datos a PostgreSQL\n",
        "\n",
        "Este notebook documenta el proceso completo de carga de datos desde archivos CSV a PostgreSQL, incluyendo el flujo de ejecuciÃ³n y los resultados obtenidos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“‹ Resumen del Proceso\n",
        "\n",
        "El proceso ETL (Extract, Transform, Load) se ejecuta en dos fases principales:\n",
        "\n",
        "1. **Fase 1: CreaciÃ³n del Esquema** - Crea todas las tablas en PostgreSQL usando SQLAlchemy ORM\n",
        "2. **Fase 2: Carga de Datos** - Lee archivos CSV, transforma los datos y los carga en las tablas creadas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”„ Flujo de EjecuciÃ³n del CÃ³digo\n",
        "\n",
        "### Punto de Entrada: `main.py`\n",
        "\n",
        "El proceso comienza en `Avance1/main.py`, que actÃºa como orquestador principal:\n",
        "\n",
        "```python\n",
        "# 1. ConfiguraciÃ³n inicial de paths\n",
        "path_manager = PathManager.get_instance()  # Singleton\n",
        "path_manager.setup_sys_path()  # Configura sys.path\n",
        "\n",
        "# 2. Import de funciones\n",
        "from Models.create_tables import create_all_tables\n",
        "from ETL.load_data import load_data\n",
        "\n",
        "# 3. EjecuciÃ³n del proceso\n",
        "def main():\n",
        "    # Paso 1: Crear esquema\n",
        "    create_all_tables()\n",
        "    \n",
        "    # Paso 2: Cargar datos\n",
        "    for config in tables_config:\n",
        "        load_data(config['file'], config['table'])\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 1: CreaciÃ³n del Esquema (`Models/create_tables.py`)\n",
        "\n",
        "**Flujo de ejecuciÃ³n:**\n",
        "\n",
        "1. **Obtener conexiÃ³n a la base de datos:**\n",
        "   ```python\n",
        "   db = DBConnector.get_instance()  # Singleton\n",
        "   engine = db.get_engine()\n",
        "   ```\n",
        "\n",
        "2. **Crear todas las tablas:**\n",
        "   ```python\n",
        "   Base.metadata.create_all(engine)\n",
        "   ```\n",
        "   - Utiliza los modelos ORM definidos en `Models/models.py`\n",
        "   - Crea 11 tablas con sus constraints, foreign keys, enums y validaciones\n",
        "   - Aplica CheckConstraints para validaciones numÃ©ricas\n",
        "   - Crea tipos ENUM nativos de PostgreSQL para campos de estado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Paso 2: Carga de Datos (`ETL/load_data.py`)\n",
        "\n",
        "**Flujo de ejecuciÃ³n para cada archivo CSV:**\n",
        "\n",
        "1. **Obtener ruta del archivo CSV:**\n",
        "   ```python\n",
        "   csv_path = path_manager.get_csv_path(file_name)\n",
        "   # Ejemplo: 'DataSet/CSV/2.Usuarios.csv'\n",
        "   ```\n",
        "\n",
        "2. **Leer archivo CSV con pandas:**\n",
        "   ```python\n",
        "   df = pd.read_csv(csv_path, encoding=ETLConfig.CSV_ENCODING)  # 'utf-8'\n",
        "   ```\n",
        "\n",
        "3. **Estandarizar nombres de columnas:**\n",
        "   ```python\n",
        "   df.columns = [clean_column_name(col) for col in df.columns]\n",
        "   # Convierte camelCase a snake_case: 'OrdenID' -> 'orden_id'\n",
        "   ```\n",
        "\n",
        "4. **Obtener conexiÃ³n a la base de datos:**\n",
        "   ```python\n",
        "   db = DBConnector.get_instance()  # Singleton\n",
        "   ```\n",
        "\n",
        "5. **Insertar datos en PostgreSQL usando COPY:**\n",
        "   ```python\n",
        "   # Obtener conexiÃ³n raw de psycopg2 usando el context manager del DBConnector\n",
        "   with db.get_raw_connection() as conn:\n",
        "       cursor = conn.cursor()\n",
        "       \n",
        "       # Convertir DataFrame a CSV en memoria (StringIO)\n",
        "       csv_buffer = io.StringIO()\n",
        "       df.to_csv(\n",
        "           csv_buffer,\n",
        "           index=False,  # No incluir el Ã­ndice\n",
        "           header=False,  # COPY no necesita header\n",
        "           sep=',',  # Separador de columnas\n",
        "           na_rep='',  # RepresentaciÃ³n de valores nulos\n",
        "           quoting=1,  # QUOTE_MINIMAL\n",
        "           escapechar='\\\\',  # CarÃ¡cter de escape\n",
        "           doublequote=True,  # Escapar comillas dobles\n",
        "           lineterminator='\\n'  # Terminador de lÃ­nea Unix\n",
        "       )\n",
        "       csv_buffer.seek(0)\n",
        "       \n",
        "       # Obtener nombres de columnas\n",
        "       columns = ', '.join(df.columns)\n",
        "       \n",
        "       # Ejecutar COPY FROM usando psycopg2\n",
        "       cursor.copy_expert(\n",
        "           f\"COPY {table_name} ({columns}) FROM STDIN WITH (FORMAT CSV, DELIMITER ',', NULL '', QUOTE '\\\"', ESCAPE '\\\\')\",\n",
        "           csv_buffer\n",
        "       )\n",
        "       \n",
        "       # Confirmar la transacciÃ³n\n",
        "       conn.commit()\n",
        "       cursor.close()\n",
        "   ```\n",
        "   - Usa el comando COPY nativo de PostgreSQL (mÃ¡xima eficiencia)\n",
        "   - Procesa todos los datos de una vez (mÃ¡s rÃ¡pido que INSERT individuales)\n",
        "   - Carga directa desde memoria sin archivos temporales\n",
        "   - Transaccional (todo o nada)\n",
        "   - Manejo automÃ¡tico de errores con rollback\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Resultado de la EjecuciÃ³n\n",
        "\n",
        "### Salida de la Consola\n",
        "\n",
        "```\n",
        "INICIANDO PROCESO ETL COMPLETO\n",
        "\n",
        "PASO 1: Creando esquema de base de datos...\n",
        "CREANDO ESQUEMA DE BASE DE DATOS CON SQLALCHEMY ORM\n",
        "Todas las tablas creadas exitosamente:\n",
        "Esquema de base de datos creado exitosamente\n",
        "\n",
        "PASO 2: Cargando datos desde archivos CSV...\n",
        "\n",
        "Leyendo archivo: 2.Usuarios.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 1000, Columnas: 5\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'usuarios' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'usuarios' (1000 filas)\n",
        "\n",
        "Leyendo archivo: 3.Categorias.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 12, Columnas: 2\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'categorias' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'categorias' (12 filas)\n",
        "\n",
        "Leyendo archivo: 4.Productos.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 36, Columnas: 5\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'productos' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'productos' (36 filas)\n",
        "\n",
        "Leyendo archivo: 5.ordenes.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 10000, Columnas: 4\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'ordenes' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'ordenes' (10000 filas)\n",
        "\n",
        "Leyendo archivo: 6.detalle_ordenes.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 10000, Columnas: 4\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'detalle_ordenes' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'detalle_ordenes' (10000 filas)\n",
        "\n",
        "Leyendo archivo: 7.direcciones_envio.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 1000, Columnas: 9\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'direcciones_envio' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'direcciones_envio' (1000 filas)\n",
        "\n",
        "Leyendo archivo: 8.carrito.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 5000, Columnas: 4\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'carrito' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'carrito' (5000 filas)\n",
        "\n",
        "Leyendo archivo: 9.metodos_pago.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 7, Columnas: 2\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'metodos_pago' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'metodos_pago' (7 filas)\n",
        "\n",
        "Leyendo archivo: 10.ordenes_metodospago.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 10000, Columnas: 3\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'ordenes_metodos_pago' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'ordenes_metodos_pago' (10000 filas)\n",
        "\n",
        "Leyendo archivo: 11.resenas_productos.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 7172, Columnas: 5\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'resenas_productos' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'resenas_productos' (7172 filas)\n",
        "\n",
        "Leyendo archivo: 12.historial_pagos.csv\n",
        "   âœ“ Archivo leÃ­do. Filas: 10000, Columnas: 5\n",
        "   âœ“ Nombres de columnas estandarizados\n",
        "   Cargando datos a la tabla 'historial_pagos' usando COPY...\n",
        "   âœ“ Datos cargados exitosamente a 'historial_pagos' (10000 filas)\n",
        "\n",
        "PROCESO ETL COMPLETADO EXITOSAMENTE\n",
        "\n",
        "Resumen:\n",
        "   - Tablas creadas: 11\n",
        "   - Archivos CSV procesados: 11\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ˆ EstadÃ­sticas del Proceso\n",
        "\n",
        "| Archivo CSV | Tabla | Filas Cargadas | Columnas |\n",
        "|-------------|-------|----------------|----------|\n",
        "| `2.Usuarios.csv` | `usuarios` | 1,000 | 5 |\n",
        "| `3.Categorias.csv` | `categorias` | 12 | 2 |\n",
        "| `4.Productos.csv` | `productos` | 36 | 5 |\n",
        "| `5.ordenes.csv` | `ordenes` | 10,000 | 4 |\n",
        "| `6.detalle_ordenes.csv` | `detalle_ordenes` | 10,000 | 4 |\n",
        "| `7.direcciones_envio.csv` | `direcciones_envio` | 1,000 | 9 |\n",
        "| `8.carrito.csv` | `carrito` | 5,000 | 4 |\n",
        "| `9.metodos_pago.csv` | `metodos_pago` | 7 | 2 |\n",
        "| `10.ordenes_metodospago.csv` | `ordenes_metodos_pago` | 10,000 | 3 |\n",
        "| `11.resenas_productos.csv` | `resenas_productos` | 7,172 | 5 |\n",
        "| `12.historial_pagos.csv` | `historial_pagos` | 10,000 | 5 |\n",
        "\n",
        "**Total:** 55,227 registros cargados en 11 tablas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ”§ Componentes TÃ©cnicos Utilizados\n",
        "\n",
        "### 1. PatrÃ³n Singleton\n",
        "- **`DBConnector`**: GestiÃ³n Ãºnica de conexiÃ³n a PostgreSQL\n",
        "  - MÃ©todo `get_raw_connection()`: Context manager para conexiones raw de psycopg2 (usado para COPY)\n",
        "- **`PathManager`**: GestiÃ³n centralizada de rutas del proyecto\n",
        "\n",
        "### 2. ConfiguraciÃ³n Centralizada\n",
        "- **`ETLConfig`**: Todos los parÃ¡metros configurables en un solo lugar\n",
        "  - `CSV_ENCODING = 'utf-8'` - Encoding de archivos CSV\n",
        "  - **Nota**: COPY de PostgreSQL no requiere parÃ¡metros de chunksize o mÃ©todo de inserciÃ³n (es mÃ¡s eficiente que INSERT individuales)\n",
        "\n",
        "### 3. Transformaciones Aplicadas\n",
        "- **EstandarizaciÃ³n de nombres**: `clean_column_name()`\n",
        "  - Convierte camelCase a snake_case\n",
        "  - Ejemplo: `OrdenID` â†’ `orden_id`\n",
        "  - Convierte a minÃºsculas\n",
        "  - Elimina caracteres especiales\n",
        "\n",
        "### 3. Carga de Datos con COPY\n",
        "- **Comando COPY nativo de PostgreSQL**: Implementado vÃ­a `psycopg2.copy_expert()`\n",
        "  - MÃ¡xima eficiencia: COPY es el mÃ©todo mÃ¡s rÃ¡pido para cargar datos\n",
        "  - Procesamiento directo: Carga desde memoria (StringIO) sin archivos temporales\n",
        "  - Transaccional: Manejo automÃ¡tico de commit/rollback\n",
        "  - Context manager: Uso de `db.get_raw_connection()` para manejo seguro de conexiones\n",
        "\n",
        "### 4. Validaciones Implementadas\n",
        "- **CheckConstraints**: Validaciones numÃ©ricas (>= 0)\n",
        "- **Enums nativos**: ValidaciÃ³n de valores de estado\n",
        "- **Foreign Keys**: Integridad referencial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ Estructura de Archivos Ejecutados\n",
        "\n",
        "```\n",
        "Avance1/\n",
        "â”œâ”€â”€ main.py                    # Orquestador principal\n",
        "â”‚\n",
        "â”œâ”€â”€ Models/\n",
        "â”‚   â”œâ”€â”€ create_tables.py      # Paso 1: CreaciÃ³n de esquema\n",
        "â”‚   â”œâ”€â”€ models.py              # Modelos ORM (11 tablas)\n",
        "â”‚   â””â”€â”€ enums.py               # Enumeraciones (EstadoOrden, EstadoPago)\n",
        "â”‚\n",
        "â”œâ”€â”€ ETL/\n",
        "â”‚   â””â”€â”€ load_data.py           # Paso 2: Carga de datos\n",
        "â”‚\n",
        "â””â”€â”€ Utils/\n",
        "    â”œâ”€â”€ path_manager.py       # GestiÃ³n de rutas (Singleton)\n",
        "    â”œâ”€â”€ config.py              # ConfiguraciÃ³n centralizada\n",
        "    â””â”€â”€ clean_column_name.py  # TransformaciÃ³n de nombres\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## âœ… Validaciones y Constraints Aplicados\n",
        "\n",
        "### CheckConstraints (Validaciones NumÃ©ricas)\n",
        "- `precio >= 0` (productos)\n",
        "- `stock >= 0` (productos)\n",
        "- `total >= 0` (ordenes)\n",
        "- `cantidad >= 0` (detalle_ordenes, carrito)\n",
        "- `precio_unitario >= 0` (detalle_ordenes)\n",
        "- `monto_pagado >= 0` (ordenes_metodos_pago)\n",
        "- `monto >= 0` (historial_pagos)\n",
        "- `calificacion >= 1 AND calificacion <= 5` (resenas_productos)\n",
        "\n",
        "### Enums Nativos de PostgreSQL\n",
        "- **`estado_orden`**: Pendiente, Enviado, Completado, Cancelado\n",
        "- **`estado_pago`**: Procesando, Pagado, Fallido, Reembolsado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ Resultado Final\n",
        "\n",
        "âœ… **11 tablas creadas exitosamente**\n",
        "\n",
        "âœ… **11 archivos CSV procesados exitosamente**\n",
        "\n",
        "âœ… **55,227 registros cargados en PostgreSQL**\n",
        "\n",
        "âœ… **Todas las validaciones y constraints aplicados correctamente**\n",
        "\n",
        "âœ… **Integridad referencial mantenida mediante Foreign Keys**\n",
        "\n",
        "âœ… **Tipos de datos ajustados para preservar integridad semÃ¡ntica**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
