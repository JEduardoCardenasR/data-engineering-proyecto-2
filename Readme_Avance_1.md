# Avance 1: Sistema ETL para Carga de Datos a PostgreSQL

## üìã Descripci√≥n General

Este proyecto implementa un sistema ETL (Extract, Transform, Load) completo para cargar datos desde archivos CSV a una base de datos PostgreSQL. El sistema utiliza **arquitectura de staging** (√°rea de preparaci√≥n) siguiendo las mejores pr√°cticas de Data Engineering:

1. **Carga datos crudos** a tablas staging (sin IDs, sin foreign keys)
2. **Aplica transformaciones** sobre los datos en staging
3. **Carga datos transformados** a producci√≥n (con IDs autoincrementales y foreign keys)

El sistema utiliza SQLAlchemy ORM para la definici√≥n de esquemas de producci√≥n, SQL directo para staging, y pandas para el procesamiento de datos, aplicando patrones de dise√±o como Singleton para la gesti√≥n de conexiones y configuraci√≥n centralizada.

### Caracter√≠sticas Principales

- **Arquitectura de Staging**: Separaci√≥n entre datos crudos y datos transformados
- **Patr√≥n Singleton**: Gesti√≥n √∫nica de conexi√≥n a base de datos mediante `DBConnector`
- **SQLAlchemy ORM**: Definici√≥n de modelos de datos para producci√≥n
- **SQL Directo**: Creaci√≥n de tablas staging sin restricciones de ORM
- **Transformaciones Autom√°ticas**: Limpieza y normalizaci√≥n de datos basadas en EDA
- **Resoluci√≥n Autom√°tica de Foreign Keys**: Mapeo autom√°tico de IDs durante la carga
- **Configuraci√≥n Centralizada**: Todos los par√°metros configurables en un solo lugar
- **Path Manager**: Gesti√≥n centralizada de rutas del proyecto con cach√©
- **Limpieza Autom√°tica**: Estandarizaci√≥n de nombres de columnas (camelCase ‚Üí snake_case)
- **Carga Optimizada**: Uso de COPY nativo de PostgreSQL para m√°xima eficiencia

### Estructura del Proyecto

```
Proyecto Integrador/
‚îú‚îÄ‚îÄ pipeline/                    # M√≥dulo principal del ETL
‚îÇ   ‚îú‚îÄ‚îÄ models/                  # Modelos ORM y creaci√≥n de tablas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Modelos ORM de producci√≥n (11 tablas)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ enums.py             # Enumeraciones para estados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ create_tables.py     # Creaci√≥n de tablas (staging y producci√≥n)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ create_staging_tables.sql  # SQL para crear tablas staging
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ etl/                     # M√≥dulo ETL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_raw_data.py     # Carga datos crudos a staging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformations.py   # Transformaciones sobre staging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ load_to_production.py # Carga a producci√≥n y resuelve FKs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pipeline.py          # Pipeline modular (ejecuci√≥n por pasos)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/                   # M√≥dulo de utilidades
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ path_manager.py      # Gesti√≥n de paths (Singleton)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuraci√≥n centralizada
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clean_column_name.py # Estandarizaci√≥n de nombres
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                  # Scripts ejecutables
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py              # Orquestador principal del ETL
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/               # Notebooks de an√°lisis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ EDA/                 # An√°lisis exploratorio de datos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ proceso_carga_datos.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py              # Paquete pipeline
‚îÇ
‚îú‚îÄ‚îÄ database/                     # Conexi√≥n a base de datos
‚îÇ   ‚îî‚îÄ‚îÄ db_connector.py          # DBConnector (Singleton)
‚îÇ
‚îú‚îÄ‚îÄ data/                         # Datos del proyecto
‚îÇ   ‚îú‚îÄ‚îÄ CSV/                     # Archivos CSV originales
‚îÇ   ‚îî‚îÄ‚îÄ sql/                     # Scripts SQL
‚îÇ
‚îî‚îÄ‚îÄ Readme_Avance_1.md           # Este archivo
```

---

## üîß Fase 1: Configuraci√≥n Inicial del Entorno

### Objetivo

Configurar el entorno de trabajo completo para el proyecto, incluyendo entorno virtual, dependencias y conexi√≥n a PostgreSQL.

### Componentes Implementados

#### 1. Entorno Virtual y Dependencias

- Entorno virtual de Python (`venv`)
- Archivo `requirements.txt` con dependencias:
  - `psycopg2-binary` (2.9.11) - Driver PostgreSQL
  - `SQLAlchemy` (2.0.44) - ORM
  - `pandas` (2.3.3) - Procesamiento de datos
  - `python-dotenv` (1.2.1) - Variables de entorno

#### 2. DBConnector (Patr√≥n Singleton)

**Ubicaci√≥n**: `database/db_connector.py`

- Implementaci√≥n del patr√≥n Singleton para conexi√≥n √∫nica
- Carga autom√°tica de variables de entorno desde `.env`
- Creaci√≥n y gesti√≥n del Engine de SQLAlchemy
- Pool pre-ping para verificaci√≥n de conexi√≥n

**Uso**:
```python
from database.db_connector import DBConnector

db = DBConnector.get_instance()
engine = db.get_engine()
```

#### 3. Variables de Entorno

Archivo `.env` en la ra√≠z del proyecto:
```env
DB_HOST=******
DB_PORT=******
DB_NAME=******
DB_USER=******
DB_PASS=******
```

### Comandos de Configuraci√≥n

**Windows PowerShell**:
```powershell
.\venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

**Linux/Mac**:
```bash
source venv/bin/activate
pip install -r requirements.txt
```

### Estado de la Fase 1

- [x] Entorno virtual creado
- [x] Dependencias instaladas
- [x] Archivo `.env` configurado
- [x] Clase `DBConnector` implementada con patr√≥n Singleton
- [x] Integraci√≥n con SQLAlchemy configurada

---

## üìä Fase 2: Refactorizaci√≥n a Arquitectura de Staging

### Objetivo

Refactorizar el proceso ETL para implementar una arquitectura de staging que siga las mejores pr√°cticas de Data Engineering:
- Cargar datos crudos sin IDs ni foreign keys
- Aplicar transformaciones sobre staging
- Cargar datos transformados a producci√≥n con generaci√≥n autom√°tica de IDs

### Flujo del Proceso ETL (6 Pasos)

```
1. Crear tablas STAGING (SQL directo, sin IDs, sin FKs)
   ‚Üì
2. Cargar datos CRUDOS a staging (desde CSV)
   ‚Üì
3. Crear tablas de PRODUCCI√ìN (ORM, con IDs, FKs, constraints)
   ‚Üì
4. Aplicar TRANSFORMACIONES sobre staging
   ‚Üì
5. Cargar datos TRANSFORMADOS a producci√≥n (con generaci√≥n de IDs)
   ‚Üì
6. Resolver FOREIGN KEYS autom√°ticamente
```

### Componentes Implementados

#### 1. Modelos ORM de Producci√≥n (`pipeline/models/models.py`)

Definici√≥n de 11 modelos de datos usando SQLAlchemy ORM para la capa de producci√≥n:

- `Usuario` - Informaci√≥n de usuarios
- `Categoria` - Categor√≠as de productos
- `Producto` - Cat√°logo de productos
- `Orden` - √ìrdenes de compra
- `DetalleOrden` - Detalles de cada orden
- `DireccionEnvio` - Direcciones de env√≠o
- `Carrito` - Carrito de compras
- `MetodoPago` - M√©todos de pago disponibles
- `OrdenMetodoPago` - Relaci√≥n orden-m√©todo de pago
- `ResenaProducto` - Rese√±as de productos
- `HistorialPago` - Historial de pagos

**Caracter√≠sticas**:
- Claves primarias auto-incrementales
- Foreign keys para relaciones entre tablas
- Constraints y validaciones (CheckConstraint)
- Enums para campos de estado
- Valores por defecto (server_default)

##### 1.1. Validaciones con CheckConstraint

Se implementaron validaciones num√©ricas a nivel de base de datos para preservar la integridad sem√°ntica:

**Validaciones de valores no negativos**:

| Tabla | Campo | Validaci√≥n | Constraint |
|-------|-------|------------|------------|
| `productos` | `precio` | `>= 0` | `check_precio_positivo` |
| `productos` | `stock` | `>= 0` | `check_stock_positivo` |
| `ordenes` | `total` | `>= 0` | `check_total_positivo` |
| `detalle_ordenes` | `cantidad` | `>= 0` | `check_cantidad_positiva` |
| `detalle_ordenes` | `precio_unitario` | `>= 0` | `check_precio_unitario_positivo` |
| `carrito` | `cantidad` | `>= 0` | `check_cantidad_carrito_positiva` |
| `ordenes_metodos_pago` | `monto_pagado` | `>= 0` | `check_monto_pagado_positivo` |
| `historial_pagos` | `monto` | `>= 0` | `check_monto_positivo` |
| `resenas_productos` | `calificacion` | `1-5` | `check_calificacion_rango` |

**Total**: 9 validaciones de integridad sem√°ntica implementadas.

##### 1.2. Enums para Campos de Estado

Se implementaron Enums nativos de PostgreSQL para campos de estado:

**Archivo**: `pipeline/models/enums.py`

**EstadoOrden** (tabla `ordenes`):
- `PENDIENTE = 'Pendiente'` (valor por defecto)
- `ENVIADO = 'Enviado'`
- `COMPLETADO = 'Completado'`
- `CANCELADO = 'Cancelado'`

**EstadoPago** (tabla `historial_pagos`):
- `PROCESANDO = 'Procesando'` (valor por defecto)
- `PAGADO = 'Pagado'`
- `FALLIDO = 'Fallido'`
- `REEMBOLSADO = 'Reembolsado'`

**Beneficios de los Enums**:
- Validaci√≥n a nivel de base de datos: PostgreSQL solo acepta valores v√°lidos
- Integridad sem√°ntica: previene valores inv√°lidos
- Mejor rendimiento: tipos ENUM nativos son m√°s eficientes que VARCHAR
- Documentaci√≥n expl√≠cita: valores permitidos est√°n claramente definidos en el c√≥digo

#### 2. Creaci√≥n de Tablas (`pipeline/models/create_tables.py`)

**Funciones implementadas**:

- `create_staging_tables()`: Crea tablas staging usando SQL directo desde `create_staging_tables.sql`
  - Tablas sin primary keys ni foreign keys
  - Solo almacenan datos crudos tal como vienen del CSV
  - Ejemplo: `usuarios_raw`, `productos_raw`, `ordenes_raw`

- `create_production_tables()`: Crea tablas de producci√≥n usando SQLAlchemy ORM
  - Tablas con IDs autoincrementales, foreign keys y constraints
  - Utiliza los modelos definidos en `models.py`

- `create_all_tables()`: Crea ambas capas (staging y producci√≥n)

**Archivo SQL**: `pipeline/models/create_staging_tables.sql`
- Define el esquema de todas las tablas staging
- Sin primary keys ni foreign keys
- Permite an√°lisis exploratorio sin restricciones

#### 3. Carga de Datos Crudos a Staging (`pipeline/etl/load_raw_data.py`)

**Funci√≥n**: `load_raw_data(file_name, table_name_raw)`

**Caracter√≠sticas**:
- Lee archivos CSV desde `data/CSV/`
- Estandariza nombres de columnas (camelCase ‚Üí snake_case)
- Filtra columnas autom√°ticamente (excluye IDs primarios si existen en CSV)
- Inserta datos usando el comando **COPY nativo de PostgreSQL** v√≠a `psycopg2` para m√°xima eficiencia
- Carga datos CRUDOS sin validaciones ni constraints

**Ventajas de COPY**:
- **M√°xima eficiencia**: COPY es el m√©todo m√°s r√°pido para cargar datos en PostgreSQL
- **Procesamiento directo**: Los datos se cargan directamente desde memoria sin archivos temporales
- **Transaccional**: Los datos se cargan en una transacci√≥n √∫nica (todo o nada)

#### 4. Transformaciones (`pipeline/etl/transformations.py`)

**Funci√≥n principal**: `apply_transformations(table_name_raw, df, **kwargs)`

**Transformaciones aplicadas por tabla**:

| Tabla | Transformaciones | Resultado |
|-------|------------------|-----------|
| `usuarios_raw` | Normalizaci√≥n de emails, trim | 379 emails normalizados |
| `categorias_raw` | Trim de nombre y descripci√≥n | - |
| `productos_raw` | Trim, validaci√≥n de precios y stock (no negativos) | - |
| `ordenes_raw` | C√°lculo de totales desde detalle_ordenes | 1,000 totales corregidos |
| `detalle_ordenes_raw` | Validaci√≥n de cantidades y precios (no negativos) | - |
| `resenas_productos_raw` | Eliminaci√≥n de duplicados por (usuario_id, producto_id) | 698 rese√±as duplicadas eliminadas |
| `direcciones_envio_raw` | Trim de todos los campos de texto | - |
| `carrito_raw` | Validaci√≥n de cantidades (no negativas) | - |
| `metodos_pago_raw` | Trim de nombre y descripci√≥n | - |
| `ordenes_metodos_pago_raw` | Validaci√≥n de montos (no negativos) | - |
| `historial_pagos_raw` | Validaci√≥n de montos (no negativos) | - |

**Funciones gen√©ricas**:
- `apply_trim()`: Elimina espacios al inicio y final de campos de texto
- `normalize_emails()`: Normaliza emails (elimina espacios, acentos, caracteres especiales)
- `remove_duplicates_by_key()`: Elimina duplicados bas√°ndose en columnas clave

#### 5. Carga a Producci√≥n (`pipeline/etl/load_to_production.py`)

**Funci√≥n principal**: `load_all_to_production()`

**Caracter√≠sticas**:
- Lee datos transformados desde tablas staging
- Genera IDs autoincrementales autom√°ticamente
- Resuelve foreign keys usando mapeos de identificadores naturales
- Respeta orden de dependencias (tablas independientes primero)

**Orden de carga** (respetando dependencias):
1. **Nivel 1** (independientes): `categorias`, `metodos_pago`, `usuarios`
2. **Nivel 2** (dependen de nivel 1): `productos`, `ordenes`
3. **Nivel 3** (dependen de nivel 2): `detalle_ordenes`, `carrito`, `direcciones_envio`, `resenas_productos`, `ordenes_metodos_pago`, `historial_pagos`

**Mapeo de IDs**:
- Usa identificadores naturales (nombre, dni, email) para crear mapeos
- Convierte referencias de staging a IDs de producci√≥n autom√°ticamente
- Ejemplo: `{'nombre': 1, 'nombre2': 2}` para categorias

#### 6. Pipeline Modular (`pipeline/etl/pipeline.py`)

**Funciones disponibles**:

- `run_full_pipeline()`: Ejecuta todo el proceso ETL completo
- `run_staging_load()`: Solo carga datos crudos a staging
- `run_transformations()`: Solo aplica transformaciones sobre staging
- `run_production_load()`: Solo carga datos transformados a producci√≥n

**√ötil para**:
- Desarrollo incremental
- Debugging
- Mantenimiento
- Reprocesamiento selectivo

#### 7. Estandarizaci√≥n de Nombres (`pipeline/utils/clean_column_name.py`)

**Funci√≥n**: `clean_column_name()`

- Convierte camelCase a snake_case (ej: `OrdenID` ‚Üí `orden_id`)
- Convierte a min√∫sculas
- Maneja transiciones de may√∫sculas/min√∫sculas
- Elimina caracteres especiales

#### 8. Configuraci√≥n Centralizada (`pipeline/utils/config.py`)

**Clase**: `ETLConfig`

**Par√°metros configurables**:
- Paths de directorios: `CSV_DIR = 'data/CSV'`, `SQL_DIR = 'data/sql'`
- Encoding: `CSV_ENCODING = 'utf-8'`

#### 9. Path Manager (`pipeline/utils/path_manager.py`)

**Clase**: `PathManager` (Singleton)

- Gestiona paths del proyecto de manera centralizada
- Evita duplicaci√≥n de c√≥digo
- Implementa cach√© para evitar rec√°lculos
- Configura `sys.path` para imports

#### 10. Orquestador Principal (`pipeline/scripts/main.py`)

**Funci√≥n**: `main()`

**Flujo completo**:
1. Crear tablas staging
2. Cargar datos crudos a staging
3. Crear tablas de producci√≥n
4. Ejecutar transformaciones sobre staging
5. Cargar datos transformados a producci√≥n (con generaci√≥n de IDs)
6. Resolver foreign keys (autom√°tico)

### Archivos CSV Procesados

| Archivo | Tabla Staging | Tabla Producci√≥n | Filas |
|---------|---------------|------------------|-------|
| `2.Usuarios.csv` | `usuarios_raw` | `usuarios` | 1,000 |
| `3.Categorias.csv` | `categorias_raw` | `categorias` | 12 |
| `4.Productos.csv` | `productos_raw` | `productos` | 36 |
| `5.ordenes.csv` | `ordenes_raw` | `ordenes` | 10,000 |
| `6.detalle_ordenes.csv` | `detalle_ordenes_raw` | `detalle_ordenes` | 10,000 |
| `7.direcciones_envio.csv` | `direcciones_envio_raw` | `direcciones_envio` | 1,000 |
| `8.carrito.csv` | `carrito_raw` | `carrito` | 5,000 |
| `9.metodos_pago.csv` | `metodos_pago_raw` | `metodos_pago` | 7 |
| `10.ordenes_metodospago.csv` | `ordenes_metodos_pago_raw` | `ordenes_metodos_pago` | 10,000 |
| `11.resenas_productos.csv` | `resenas_productos_raw` | `resenas_productos` | 6,474* |
| `12.historial_pagos.csv` | `historial_pagos_raw` | `historial_pagos` | 10,000 |

*Despu√©s de eliminar 698 rese√±as duplicadas

**Total**: 
- 55,227 registros cargados a staging (datos crudos)
- 64,474 registros transformados
- 64,474 registros cargados a producci√≥n (con IDs autoincrementales)

### Ejecuci√≥n del Proceso ETL

**Opci√≥n 1: Script principal**:
```bash
# Desde la ra√≠z del proyecto
python pipeline/scripts/main.py

# O como m√≥dulo
python -m pipeline.scripts.main
```

**Opci√≥n 2: Pipeline modular**:
```python
from pipeline.etl.pipeline import run_full_pipeline

# Ejecutar todo el proceso
id_mappings = run_full_pipeline()

# O ejecutar por pasos
from pipeline.etl.pipeline import (
    run_staging_load,
    run_transformations,
    run_production_load
)

run_staging_load()
staging_data = run_transformations()
id_mappings = run_production_load()
```

### Flujo del Proceso Detallado

```
pipeline/scripts/main.py
  ‚îÇ
  ‚îú‚îÄ‚Üí Paso 1: models.create_staging_tables()
  ‚îÇ   ‚îî‚îÄ‚Üí Lee create_staging_tables.sql y ejecuta SQL directo
  ‚îÇ       ‚îî‚îÄ‚Üí Crea 11 tablas staging (sin IDs, sin FKs)
  ‚îÇ
  ‚îú‚îÄ‚Üí Paso 2: etl.load_raw_data() para cada CSV
  ‚îÇ   ‚îú‚îÄ‚Üí Lee CSV con pandas (usa utils.path_manager)
  ‚îÇ   ‚îú‚îÄ‚Üí Limpia nombres de columnas (usa utils.clean_column_name)
  ‚îÇ   ‚îú‚îÄ‚Üí Filtra columnas (excluye IDs primarios)
  ‚îÇ   ‚îî‚îÄ‚Üí Inserta datos en staging usando COPY (m√°xima eficiencia)
  ‚îÇ
  ‚îú‚îÄ‚Üí Paso 3: models.create_production_tables()
  ‚îÇ   ‚îî‚îÄ‚Üí Crea 11 tablas de producci√≥n usando ORM
  ‚îÇ       ‚îî‚îÄ‚Üí Con IDs autoincrementales, FKs y constraints
  ‚îÇ
  ‚îú‚îÄ‚Üí Paso 4: etl.transformations.apply_transformations()
  ‚îÇ   ‚îú‚îÄ‚Üí Lee datos de staging
  ‚îÇ   ‚îú‚îÄ‚Üí Aplica transformaciones espec√≠ficas por tabla
  ‚îÇ   ‚îî‚îÄ‚Üí Actualiza staging con datos transformados
  ‚îÇ
  ‚îî‚îÄ‚Üí Paso 5-6: etl.load_to_production.load_all_to_production()
      ‚îú‚îÄ‚Üí Lee datos transformados de staging
      ‚îú‚îÄ‚Üí Genera IDs autoincrementales
      ‚îú‚îÄ‚Üí Resuelve foreign keys usando mapeos
      ‚îî‚îÄ‚Üí Inserta datos en producci√≥n
```

### Estado de la Fase 2

- [x] Arquitectura de staging implementada
- [x] Modelos ORM de producci√≥n definidos (11 tablas)
- [x] Tablas staging creadas con SQL directo
- [x] Funci√≥n de carga de datos crudos a staging implementada
- [x] M√≥dulo de transformaciones implementado
- [x] Funci√≥n de carga a producci√≥n con resoluci√≥n de FKs implementada
- [x] Pipeline modular implementado
- [x] Estandarizaci√≥n de nombres de columnas
- [x] Configuraci√≥n centralizada
- [x] Path Manager implementado
- [x] Orquestador principal funcional
- [x] Proceso ETL completo probado
- [x] Validaciones num√©ricas con CheckConstraint (9 validaciones)
- [x] Enums para campos de estado (EstadoOrden, EstadoPago)
- [x] Resoluci√≥n autom√°tica de foreign keys
- [x] Mapeo de IDs usando identificadores naturales

---

## üéØ Patrones de Dise√±o Aplicados

1. **Singleton**: `DBConnector` y `PathManager` para instancias √∫nicas
2. **Configuraci√≥n Centralizada**: `ETLConfig` para valores configurables
3. **Separaci√≥n de Responsabilidades**: Organizaci√≥n modular por funci√≥n
   - `models/`: Modelos ORM, enumeraciones y creaci√≥n de esquema
   - `etl/`: Proceso de carga de datos desde CSV (con staging)
   - `utils/`: Utilidades compartidas (paths, configuraci√≥n, transformaciones)
   - `scripts/`: Scripts ejecutables
4. **Arquitectura de Capas**: Separaci√≥n entre staging (raw) y producci√≥n (transformada)

---

## üîÑ Ventajas del Nuevo Flujo con Staging

1. **Separaci√≥n de responsabilidades**: Datos crudos vs datos transformados
2. **An√°lisis exploratorio**: Puedes analizar datos crudos sin restricciones
3. **Reprocesamiento**: F√°cil reprocesar si hay errores
4. **Trazabilidad**: Puedes comparar staging vs producci√≥n
5. **Mejores pr√°cticas**: Sigue el patr√≥n est√°ndar de Data Warehousing
6. **Flexibilidad**: Permite ejecutar el pipeline por pasos
7. **Debugging**: Facilita identificar problemas en cada etapa

---

## üìö Referencias

- [SQLAlchemy Documentation](https://docs.sqlalchemy.org/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [Patr√≥n Singleton en Python](https://refactoring.guru/design-patterns/singleton/python/example)
- [Data Warehousing Best Practices](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/)

---

**Fecha de Documentaci√≥n**: Enero 2025  
**Versi√≥n**: 3.0 - Refactorizaci√≥n a Arquitectura de Staging Completada
